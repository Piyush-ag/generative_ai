{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
      "metadata": {
        "id": "b076bd1a-b236-4fbc-953d-8295b25122ae"
      },
      "source": [
        "# Wine Review GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
      "metadata": {
        "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
        "tags": []
      },
      "source": [
        "In this notebook, we'll walk through the steps required to train your own GPT model on the wine review dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6cb7c7-d3d5-4b12-b357-1f6118edffe0",
      "metadata": {
        "id": "3e6cb7c7-d3d5-4b12-b357-1f6118edffe0"
      },
      "source": [
        "The code is adapted from the recommended text book [Generative Deep Learning](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/09_transformer/gpt/gpt.ipynb) 2nd Edition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i8j3ul1Dcl6D",
      "metadata": {
        "id": "i8j3ul1Dcl6D"
      },
      "source": [
        "<font color='red'>Change the runtime to GPU in the upper right corner for this exercise. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
      "metadata": {
        "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5"
      },
      "source": [
        "## 0. Parameters <a name=\"parameters\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73350761-bef2-4e96-b3ac-a158eabd2b65",
      "metadata": {
        "id": "73350761-bef2-4e96-b3ac-a158eabd2b65",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
      "metadata": {
        "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 2\n",
        "FEED_FORWARD_DIM = 256\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7716fac-0010-49b0-b98e-53be2259edde",
      "metadata": {
        "id": "b7716fac-0010-49b0-b98e-53be2259edde"
      },
      "source": [
        "## 1. Load the data <a name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9VAWPFpOezyq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9VAWPFpOezyq",
        "outputId": "dbd4d170-2f79-4fab-8242-527fb45d325a"
      },
      "outputs": [],
      "source": [
        "# mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0YnVE7omwUn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Q0YnVE7omwUn",
        "outputId": "696766dc-8b6d-42c2-ace7-08ee8f8b9674"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jIeXGPx7etwn",
      "metadata": {
        "id": "jIeXGPx7etwn"
      },
      "source": [
        "<font color='red'> Instruction: replace the path with the correct one on your Google Drive. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
      "metadata": {
        "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load the full dataset\n",
        "with open(\"/content/winemag-data-130k-v2.json\") as json_data:\n",
        "    wine_data = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db5c0fc-0d5f-42ab-ade1-57e594c416ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2db5c0fc-0d5f-42ab-ade1-57e594c416ec",
        "outputId": "80023f9e-731d-48cc-fbb2-3290327e161c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# print the first 3 records\n",
        "wine_data[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361",
      "metadata": {
        "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Filter the dataset, only includes entries where country, province, variety and description are not None\n",
        "filtered_data = [\n",
        "    \"wine review : \"\n",
        "    + x[\"country\"]\n",
        "    + \" : \"\n",
        "    + x[\"province\"]\n",
        "    + \" : \"\n",
        "    + x[\"variety\"]\n",
        "    + \" : \"\n",
        "    + x[\"description\"]\n",
        "    for x in wine_data\n",
        "    if x[\"country\"] is not None\n",
        "    and x[\"province\"] is not None\n",
        "    and x[\"variety\"] is not None\n",
        "    and x[\"description\"] is not None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
        "outputId": "5fdd13ed-6809-44c3-eaa5-faf98711904c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Count the recipes\n",
        "n_wines = len(filtered_data)\n",
        "print(f\"{n_wines} recipes loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
        "outputId": "4c24f050-6966-4be4-e770-e9a6eaa1b347"
      },
      "outputs": [],
      "source": [
        "example = filtered_data[0]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
      "metadata": {
        "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1"
      },
      "source": [
        "## 2. Tokenize the data <a name=\"tokenize\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc",
      "metadata": {
        "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
        "outputId": "5fe94171-fe17-4a04-9bd5-9763e2fbc6e0"
      },
      "outputs": [],
      "source": [
        "# Display an example of a recipe\n",
        "example_data = text_data[0]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
      "metadata": {
        "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de",
      "metadata": {
        "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de"
      },
      "outputs": [],
      "source": [
        "# Create a vectorization layer using https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6dd34a-d905-497b-926a-405380ebcf98",
      "metadata": {
        "id": "4d6dd34a-d905-497b-926a-405380ebcf98"
      },
      "outputs": [],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
        "outputId": "5701f49c-b121-48ad-904c-8ba992b5aa36"
      },
      "outputs": [],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
        "outputId": "32a64b59-83ab-4c0c-a682-a04951fbc4bf"
      },
      "outputs": [],
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c195efb-84c6-4be0-a989-a7542188ad35",
      "metadata": {
        "id": "8c195efb-84c6-4be0-a989-a7542188ad35"
      },
      "source": [
        "## 3. Create the Training Set <a name=\"create\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
      "metadata": {
        "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b"
      },
      "outputs": [],
      "source": [
        "# Create the training set of recipes and the same text shifted by one word\n",
        "# such that the predictions (y) for position i can depend only on the known outputs (x) at positions less than i\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad80ffb-4298-4249-86b4-9918d62534c5",
      "metadata": {
        "id": "cad80ffb-4298-4249-86b4-9918d62534c5"
      },
      "outputs": [],
      "source": [
        "example_input_output = train_ds.take(1).get_single_element()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ff7263-f62d-44c1-997b-1aa99a393521",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "67ff7263-f62d-44c1-997b-1aa99a393521",
        "outputId": "84c30349-6057-4846-e8ab-828a49005275"
      },
      "outputs": [],
      "source": [
        "# Example Input\n",
        "example_input_output[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2e2cad-414c-4e6d-a2ac-6b9598f9dd01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ef2e2cad-414c-4e6d-a2ac-6b9598f9dd01",
        "outputId": "2b56fb09-3034-49e5-8742-95872aaeac54"
      },
      "outputs": [],
      "source": [
        "# Example Output (shifted by one token)\n",
        "example_input_output[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
      "metadata": {
        "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
        "tags": []
      },
      "source": [
        "## 4. Create the causal attention mask function <a name=\"causal\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554a4184-61c2-4eb7-a063-d965586a8188",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "554a4184-61c2-4eb7-a063-d965586a8188",
        "outputId": "8cd8c6df-c24f-4ace-e130-9c38a8cc0ce0"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3501dbad-0860-40ad-b7d6-47950e37858f",
      "metadata": {
        "id": "3501dbad-0860-40ad-b7d6-47950e37858f"
      },
      "source": [
        "## 5. Create a Transformer Block layer <a name=\"transformer\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5285a1cb-fce1-46b1-b088-b596002fa9ae",
      "metadata": {
        "id": "5285a1cb-fce1-46b1-b088-b596002fa9ae"
      },
      "outputs": [],
      "source": [
        "# MultiHeadAttention layer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention\n",
        "# Dropout layer and Dense layer (feed-foward layer) as you learnt in AI Essentials course\n",
        "# LayerNormalization: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn = layers.MultiHeadAttention(\n",
        "            num_heads, key_dim, output_shape=embed_dim\n",
        "        )\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(\n",
        "            batch_size, seq_len, seq_len, tf.bool\n",
        "        )\n",
        "        attention_output, attention_scores = self.attn(\n",
        "            inputs,\n",
        "            inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076a6be0-9796-4974-9bcd-6ebbcfe7514e",
      "metadata": {
        "id": "076a6be0-9796-4974-9bcd-6ebbcfe7514e",
        "tags": []
      },
      "source": [
        "## 6. Create the Token and Position Embedding <a name=\"embedder\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf5cb25-88ae-4026-9e21-c1e6b5094a2c",
      "metadata": {
        "id": "fdf5cb25-88ae-4026-9e21-c1e6b5094a2c"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, max_len, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_len\": self.max_len,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac2e2d4-5980-47e3-b5b0-6c41c0c2d152",
      "metadata": {
        "id": "aac2e2d4-5980-47e3-b5b0-6c41c0c2d152"
      },
      "source": [
        "## 7. Build the Transformer model <a name=\"transformer_decoder\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c57596e-e17d-4959-b6e8-7581b0bace3a",
      "metadata": {
        "id": "8c57596e-e17d-4959-b6e8-7581b0bace3a"
      },
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
        ")(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
        "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1c3b0f-3382-444d-bb04-bae143ae5d61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "1a1c3b0f-3382-444d-bb04-bae143ae5d61",
        "outputId": "78eabdaf-495d-4ac2-8ab8-f55e0236d1c9"
      },
      "outputs": [],
      "source": [
        "gpt.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b14665-4359-447b-be58-3fd58ba69084",
      "metadata": {
        "id": "35b14665-4359-447b-be58-3fd58ba69084"
      },
      "source": [
        "## 8. Train the Transformer <a name=\"train\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
      "metadata": {
        "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "# A checkpoint is a saved state of a model, specifically the values of model variables (weights and biases) at a given point during training.\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y, att = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": start_prompt,\n",
        "                    \"word_probs\": probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"wine review\", max_tokens=80, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349865fe-ffbe-450e-97be-043ae1740e78",
      "metadata": {
        "id": "349865fe-ffbe-450e-97be-043ae1740e78"
      },
      "outputs": [],
      "source": [
        "# Create a model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
        "outputId": "2c2444f3-54f8-4c5a-989e-3126112f6d13",
        "tags": []
      },
      "outputs": [],
      "source": [
        "gpt.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PLk1UlASfB3c",
      "metadata": {
        "id": "PLk1UlASfB3c"
      },
      "source": [
        "<font color='red'>Instruction: do not forget to replace the path.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
      "metadata": {
        "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "gpt.save(\"/gpt.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20",
      "metadata": {
        "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20"
      },
      "source": [
        "## 9. Generate text using the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649",
      "metadata": {
        "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649"
      },
      "outputs": [],
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        highlighted_text = []\n",
        "        for word, att_score in zip(\n",
        "            i[\"prompt\"].split(), np.mean(i[\"atts\"], axis=0)\n",
        "        ):\n",
        "            highlighted_text.append(\n",
        "                '<span style=\"background-color:rgba(135,206,250,'\n",
        "                + str(att_score / max(np.mean(i[\"atts\"], axis=0)))\n",
        "                + ');\">'\n",
        "                + word\n",
        "                + \"</span>\"\n",
        "            )\n",
        "        highlighted_text = \" \".join(highlighted_text)\n",
        "        display(HTML(highlighted_text))\n",
        "\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
        "outputId": "6905a735-2c0b-4314-9d87-8425f3b4a18c"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : us\", max_tokens=80, temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_81xnKVHitfT",
      "metadata": {
        "id": "_81xnKVHitfT"
      },
      "source": [
        "<font color='red'>Instruction: Now try it yourself with a different starting context, number of tokens, and temperature. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G07uCts_jDZC",
      "metadata": {
        "id": "G07uCts_jDZC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heY8kLk8ii4_",
      "metadata": {
        "id": "heY8kLk8ii4_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fkuXnc1Fg0xM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5731
        },
        "id": "fkuXnc1Fg0xM",
        "outputId": "e552fe9c-4588-4f33-eb56-993dae30f0a0"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : germany\", max_tokens=50, temperature=0.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UrEgNlIWhSVv",
      "metadata": {
        "id": "UrEgNlIWhSVv"
      },
      "source": [
        "## Homework Questions\n",
        "1. Generate a wine review starting with 'wine review: italy', using 20 tokens and a temperature of 1.0. Print the probability distribution of the words.\n",
        "2. Generate another wine review starting with 'wine review: italy', using 20 tokens and a temperature of 0.1. Print the probability distribution of the words.\n",
        "\n",
        "In a Word document, analyze the first five generation steps at both temperatures:\n",
        "1. What is the next-token distribution?\n",
        "2. Which context words receive higher attention when generating this token?\n",
        "3. How do different temperatures impact the next-token distribution?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae2da8e-9b7c-4b71-b37b-021115b3d7ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2308
        },
        "id": "4ae2da8e-9b7c-4b71-b37b-021115b3d7ea",
        "outputId": "f0efc238-c525-43bf-d4f6-b14be45dc1bf"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : Italy\", max_tokens=20, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cae6d5d-263d-4455-b96c-f315cbe284ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2328
        },
        "id": "5cae6d5d-263d-4455-b96c-f315cbe284ee",
        "outputId": "a211f357-2556-44e8-cd63-c554889fd5f5"
      },
      "outputs": [],
      "source": [
        "info = text_generator.generate(\n",
        "    \"wine review : Italy\", max_tokens=20, temperature=0.1\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
